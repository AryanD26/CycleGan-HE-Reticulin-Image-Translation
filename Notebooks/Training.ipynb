{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a1477-58f1-4a64-97fd-542bbd7e8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 1: Project Setup, Configuration, and Path Management\n",
    "# ==============================================================================\n",
    "# This cell imports all necessary libraries and defines all the high-level\n",
    "# parameters and paths for the training run. It is the main control panel\n",
    "# for any experiment.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Core Libraries ---\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "# --- Deep Learning & Data Processing Libraries ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch_fidelity import calculate_metrics\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. EXPERIMENT CONFIGURATION\n",
    "# ==============================================================================\n",
    "# All user-configurable parameters are grouped here for easy access.\n",
    "\n",
    "# --- Training Control ---\n",
    "LOAD_MODEL = False                # Set to True to resume training from a checkpoint\n",
    "EPOCH_TO_LOAD_FROM = 0          # The epoch number of the checkpoint to load\n",
    "NUM_EPOCHS = 50                  # The total number of epochs to train for\n",
    "\n",
    "# --- Model & Training Hyperparameters ---\n",
    "BATCH_SIZE = 16                  # Number of images per training step. Adjust based on GPU memory.\n",
    "IMG_SIZE = 512                   # All images will be resized to this dimension.\n",
    "IMG_CHANNELS = 3                 # Number of channels for the images (3 for RGB).\n",
    "LEARNING_RATE_GEN = 2e-4         # Learning rate for the Generator's Adam optimizer.\n",
    "LEARNING_RATE_DISC = 2e-4        # Learning rate for the Discriminator's Adam optimizer.\n",
    "LAMBDA_CYCLE = 10.0              # Weight for the cycle-consistency loss.\n",
    "LAMBDA_IDENTITY = 5.0            # Weight for the identity loss (0.5 * LAMBDA_CYCLE).\n",
    "STEPS_PER_EPOCH = 2000           # Number of batches to process per \"epoch\" for faster feedback.\n",
    "\n",
    "# --- Logging & Saving Frequency ---\n",
    "SAVE_MODEL = True                # Set to True to save model checkpoints.\n",
    "SAVE_MODEL_EVERY_N_EPOCHS = 2    # How often to save a checkpoint.\n",
    "SAVE_SAMPLES_EVERY_N_EPOCHS = 5  # How often to save generated sample images.\n",
    "CALC_METRICS_EVERY_N_EPOCHS = 5  # How often to calculate FID and plot loss graphs.\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. SYSTEM & PATH SETUP (AUTOMATED)\n",
    "# ==============================================================================\n",
    "# This section automatically sets up devices, paths, and logging based on the\n",
    "# configuration above. No user changes should be needed here.\n",
    "\n",
    "# --- Device Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Directory & Path Definitions (Using H&E / Reticulin Naming) ---\n",
    "NOTEBOOK_CWD = os.getcwd()\n",
    "H_AND_E_BASE_DIR = \"H&E_split_dataset\"\n",
    "RETICULIN_BASE_DIR = \"Retic_split_dataset\"\n",
    "\n",
    "PATH_TRAIN_H_FOLDER = os.path.join(NOTEBOOK_CWD, H_AND_E_BASE_DIR, \"train\")\n",
    "PATH_TEST_H_FOLDER  = os.path.join(NOTEBOOK_CWD, H_AND_E_BASE_DIR, \"test\")\n",
    "PATH_TRAIN_R_FOLDER = os.path.join(NOTEBOOK_CWD, RETICULIN_BASE_DIR, \"train\")\n",
    "PATH_TEST_R_FOLDER  = os.path.join(NOTEBOOK_CWD, RETICULIN_BASE_DIR, \"test\")\n",
    "\n",
    "# --- Dynamic Naming for Outputs ---\n",
    "# Creates a unique ID for this run to keep outputs organized.\n",
    "run_id_string = f\"cyclegan_bs{BATCH_SIZE}_img{IMG_SIZE}\"\n",
    "OUTPUT_IMAGE_DIR = os.path.join(NOTEBOOK_CWD, \"saved_images\", run_id_string)\n",
    "CHECKPOINT_SAVE_DIR = os.path.join(NOTEBOOK_CWD, \"checkpoints\", run_id_string)\n",
    "LOG_DIR = os.path.join(NOTEBOOK_CWD, \"logs\")\n",
    "\n",
    "# Base filenames for checkpoints\n",
    "CHECKPOINT_GEN_H_BASE = f\"genh_{run_id_string}.pth.tar\"\n",
    "CHECKPOINT_GEN_R_BASE = f\"genr_{run_id_string}.pth.tar\"\n",
    "CHECKPOINT_DISC_H_BASE = f\"disch_{run_id_string}.pth.tar\"\n",
    "CHECKPOINT_DISC_R_BASE = f\"discr_{run_id_string}.pth.tar\"\n",
    "\n",
    "# --- Create Output Directories ---\n",
    "os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. INITIALIZATION & PRE-RUN CHECKS\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Configure Logging ---\n",
    "log_filename = f\"train_log_{run_id_string}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "log_filepath = os.path.join(LOG_DIR, log_filename)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filepath),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"--- Starting New Training Session ---\")\n",
    "logging.info(f\"Log file will be saved to: {os.path.abspath(log_filepath)}\")\n",
    "\n",
    "# --- Log System & Hyperparameter Details ---\n",
    "logging.info(f\"--- Device Setup ---\")\n",
    "logging.info(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    logging.info(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "logging.info(f\"--- Key Hyperparameters ---\")\n",
    "logging.info(f\"Batch Size: {BATCH_SIZE}, Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "logging.info(f\"Learning Rates (Gen/Disc): {LEARNING_RATE_GEN}/{LEARNING_RATE_DISC}\")\n",
    "logging.info(f\"Total Epochs: {NUM_EPOCHS}, Steps per Epoch: {STEPS_PER_EPOCH}\")\n",
    "\n",
    "# --- Verify Dataset Paths ---\n",
    "logging.info(\"\\n--- Verifying Dataset Paths ---\")\n",
    "all_paths_ok = True\n",
    "for domain_label, path_folder in [(\"Train H&E\", PATH_TRAIN_H_FOLDER), (\"Train Reticulin\", PATH_TRAIN_R_FOLDER), (\"Test H&E\", PATH_TEST_H_FOLDER), (\"Test Reticulin\", PATH_TEST_R_FOLDER)]:\n",
    "    if not os.path.isdir(path_folder):\n",
    "        logging.error(f\"CRITICAL ERROR: {domain_label} folder NOT FOUND at: {os.path.abspath(path_folder)}\")\n",
    "        all_paths_ok = False\n",
    "    else:\n",
    "        num_files = len(glob.glob(os.path.join(path_folder, \"*.tif\")))\n",
    "        logging.info(f\"SUCCESS: Found {domain_label} folder with {num_files} '.tif' files.\")\n",
    "        if num_files == 0:\n",
    "            logging.warning(f\"WARNING: Folder for {domain_label} exists, but contains no '.tif' files.\")\n",
    "if not all_paths_ok:\n",
    "    raise FileNotFoundError(\"One or more essential data folders are missing. Please check the paths defined in Cell 1.\")\n",
    "\n",
    "# --- Set Seed for Reproducibility ---\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "logging.info(\"\\n--- Cell 1 Setup Complete. Ready to proceed. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ee918-22e6-49eb-9773-fc79d1467aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 2: Data Loading Pipeline (Final Clean Version)\n",
    "# ==============================================================================\n",
    "# This cell defines the process for loading images from disk and preparing them\n",
    "# for the model. It assumes all necessary libraries were imported in Cell 1.\n",
    "# It is optimized for performance by deferring heavy transformations (like resizing)\n",
    "# to the GPU, which will be handled in the training loop (Cell 5).\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Utility Function: Image Loading ---\n",
    "def load_tiff_to_tensor_raw(path, target_channels):\n",
    "    try:\n",
    "        img_np = tifffile.imread(path)\n",
    "        if img_np is None: raise IOError(\"tifffile.imread returned None\")\n",
    "        if img_np.ndim == 2:  img_np = np.stack([img_np] * 3, axis=-1)\n",
    "        elif img_np.shape[-1] == 4:  img_np = img_np[..., :3]\n",
    "        elif img_np.shape[-1] == 1:  img_np = np.concatenate([img_np] * 3, axis=-1)\n",
    "        if img_np.dtype == np.uint8:\n",
    "            tensor = torch.from_numpy(img_np.astype(np.float32)).permute(2, 0, 1) / 255.0\n",
    "        else:\n",
    "            tensor = torch.from_numpy(img_np.astype(np.float32)).permute(2, 0, 1) / 65535.0\n",
    "        return (tensor * 2.0) - 1.0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LOAD_TENSOR_ERROR for {path}: {e}\")\n",
    "        return None # Return None on failure, handled by the caller\n",
    "\n",
    "# --- Dataset Class Definition ---\n",
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root_H_folder, root_R_folder, domain_name, steps_per_epoch=None):\n",
    "        self.paths_H = sorted(glob.glob(os.path.join(root_H_folder, \"*.tif\")))\n",
    "        self.paths_R = sorted(glob.glob(os.path.join(root_R_folder, \"*.tif\")))\n",
    "        self.len_H = len(self.paths_H)\n",
    "        self.len_R = len(self.paths_R)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        logging.info(f\"--- {domain_name} Dataset Initialized ---\")\n",
    "        logging.info(f\"Found {self.len_H} H&E images and {self.len_R} Reticulin images.\")\n",
    "\n",
    "        if self.len_H == 0 or self.len_R == 0:\n",
    "            self.length = 0; raise ValueError(f\"CRITICAL: {domain_name} dataset is empty.\")\n",
    "        elif self.steps_per_epoch is not None:\n",
    "            self.length = self.steps_per_epoch * BATCH_SIZE\n",
    "            logging.info(f\"Using fixed steps per epoch. Epoch will contain {self.length} items.\")\n",
    "        else: self.length = max(self.len_H, self.len_R)\n",
    "\n",
    "    def __len__(self): return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This retry loop is robust against corrupt files found during training.\n",
    "        while True:\n",
    "            img_H_path = self.paths_H[random.randint(0, self.len_H - 1)]\n",
    "            img_R_path = self.paths_R[random.randint(0, self.len_R - 1)]\n",
    "            img_H_tensor = load_tiff_to_tensor_raw(img_H_path, IMG_CHANNELS)\n",
    "            img_R_tensor = load_tiff_to_tensor_raw(img_R_path, IMG_CHANNELS)\n",
    "            if img_H_tensor is not None and img_R_tensor is not None:\n",
    "                return img_H_tensor, img_R_tensor\n",
    "            # If a file fails to load, the loop continues and gets a new random pair.\n",
    "\n",
    "# --- DataLoader Instantiation ---\n",
    "num_dataloader_workers = 0 \n",
    "logging.info(f\"Using num_workers = {num_dataloader_workers} for DataLoaders.\")\n",
    "\n",
    "train_dataset = PairedImageDataset(\n",
    "    root_H_folder=PATH_TRAIN_H_FOLDER, root_R_folder=PATH_TRAIN_R_FOLDER, \n",
    "    domain_name=\"Train\", steps_per_epoch=STEPS_PER_EPOCH\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=num_dataloader_workers, pin_memory=True, drop_last=True\n",
    ")\n",
    "logging.info(f\"Training DataLoader created with {len(train_dataloader)} batches per epoch.\")\n",
    "\n",
    "vis_dataset = PairedImageDataset(\n",
    "    root_H_folder=PATH_TEST_H_FOLDER, root_R_folder=PATH_TEST_R_FOLDER, \n",
    "    domain_name=\"Visualization\"\n",
    ")\n",
    "vis_dataloader = DataLoader(\n",
    "    vis_dataset, batch_size=16, shuffle=False, \n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "logging.info(f\"Visualization DataLoader created with {len(vis_dataloader)} total batches.\")\n",
    "\n",
    "logging.info(\"\\n--- Cell 2 Data Pipeline Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4d822-2e24-4c51-a847-c6e4a4259767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 3: Model Architectures (U-Net as Generator and PatchGAN as Discriminator)\n",
    "# ==============================================================================\n",
    "# This cell defines the neural network architectures. The Generator now uses a\n",
    "# U-Net architecture with skip connections, which is excellent for tasks\n",
    "# requiring high-resolution detail preservation.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Helper Block for U-Net ---\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"A standard Down-Convolution or Up-Convolution block.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
    "        else:\n",
    "            self.conv = nn.ConvTranspose2d(in_channels, out_channels, **kwargs)\n",
    "        \n",
    "        self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "# --- 1. GENERATOR (U-Net Architecture) ---\n",
    "class UNetGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    The Generator network based on the U-Net architecture. It uses an\n",
    "    encoder-decoder structure with skip connections to pass spatial information\n",
    "    from the downsampling path to the upsampling path, preserving details.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_channels=3, features=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Encoder (Downsampling Path) ---\n",
    "        self.encoder1 = ConvBlock(img_channels, features, kernel_size=4, stride=2, padding=1) # 64x256x256\n",
    "        self.encoder2 = ConvBlock(features, features * 2, kernel_size=4, stride=2, padding=1) # 128x128x128\n",
    "        self.encoder3 = ConvBlock(features * 2, features * 4, kernel_size=4, stride=2, padding=1) # 256x64x64\n",
    "        self.encoder4 = ConvBlock(features * 4, features * 8, kernel_size=4, stride=2, padding=1) # 512x32x32\n",
    "\n",
    "        # --- Bottleneck ---\n",
    "        self.bottleneck = ConvBlock(features * 8, features * 8, kernel_size=4, stride=2, padding=1) # 512x16x16\n",
    "        # Note: Added one more bottleneck layer for symmetry if input is 512\n",
    "        self.bottleneck2 = ConvBlock(features*8, features*8, kernel_size=4, stride=2, padding=1) # 512x8x8\n",
    "\n",
    "        # --- Decoder (Upsampling Path) ---\n",
    "        self.up0 = ConvBlock(features*8, features*8, down=False, kernel_size=4, stride=2, padding=1)\n",
    "        self.up1 = ConvBlock(features * 8 * 2, features * 8, down=False, kernel_size=4, stride=2, padding=1)\n",
    "        self.up2 = ConvBlock(features * 8 * 2, features * 4, down=False, kernel_size=4, stride=2, padding=1)\n",
    "        self.up3 = ConvBlock(features * 4 * 2, features * 2, down=False, kernel_size=4, stride=2, padding=1)\n",
    "        self.up4 = ConvBlock(features * 2 * 2, features, down=False, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        # --- Final Output Layer ---\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(), # Tanh squashes output to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through encoder, saving outputs for skip connections\n",
    "        d1 = self.encoder1(x)\n",
    "        d2 = self.encoder2(d1)\n",
    "        d3 = self.encoder3(d2)\n",
    "        d4 = self.encoder4(d3)\n",
    "        b1 = self.bottleneck(d4)\n",
    "        b2 = self.bottleneck2(b1)\n",
    "\n",
    "        # Pass through decoder, concatenating skip connections\n",
    "        u0 = self.up0(b2)\n",
    "        u1 = self.up1(torch.cat([u0, b1], dim=1))\n",
    "        u2 = self.up2(torch.cat([u1, d4], dim=1))\n",
    "        u3 = self.up3(torch.cat([u2, d3], dim=1))\n",
    "        u4 = self.up4(torch.cat([u3, d2], dim=1))\n",
    "        return self.final(torch.cat([u4, d1], dim=1))\n",
    "\n",
    "# --- 2. DISCRIMINATOR (PatchGAN - Unchanged) ---\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"The PatchGAN Discriminator network (critic).\"\"\"\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self._discriminator_block(features[0], features[1], stride=2),\n",
    "            self._discriminator_block(features[1], features[2], stride=2),\n",
    "            self._discriminator_block(features[2], features[3], stride=1), # Last block has stride 1\n",
    "            nn.Conv2d(features[3], 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")\n",
    "        )\n",
    "    def _discriminator_block(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "logging.info(\"Model architectures defined (U-Net Generator, PatchGAN Discriminator).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d40128-e449-4d12-8b18-001fdacadbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 4: Model and Training Initialization\n",
    "# ==============================================================================\n",
    "# This cell instantiates all the necessary components for training:\n",
    "# 1. Loss Functions: Defines the objectives for the networks.\n",
    "# 2. Models: Creates instances of the U-Net Generator and PatchGAN Discriminator.\n",
    "# 3. Optimizers: Defines how the model weights are updated.\n",
    "# 4. Utilities: Sets up the Replay Buffer and Checkpoint functions.\n",
    "# 5. Resume Logic: Handles loading a saved model to continue training.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Loss Function Definitions ---\n",
    "# The criteria used to measure model performance and calculate gradients.\n",
    "\n",
    "# Adversarial Loss: Measures how well the generator fools the discriminator.\n",
    "# MSELoss is a common choice for this in many GAN implementations.\n",
    "adv_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Cycle-Consistency Loss: Measures the difference between an original image and\n",
    "# its \"round-trip\" reconstruction. L1Loss (Mean Absolute Error) encourages\n",
    "# less blurry results than MSELoss.\n",
    "cycle_loss_fn = nn.L1Loss()\n",
    "\n",
    "# Identity Loss: A regularization term that encourages the generator to not\n",
    "# change an image that is already in the target domain.\n",
    "identity_loss_fn = nn.L1Loss()\n",
    "\n",
    "logging.info(\"Loss functions defined (Adversarial, Cycle, Identity).\")\n",
    "\n",
    "\n",
    "# --- 2. Model, Optimizer, and Scaler Initialization ---\n",
    "\n",
    "# <<< --- THIS IS THE KEY CHANGE --- >>>\n",
    "# Instantiate the U-Net generators and the discriminators from Cell 3.\n",
    "# gen_H translates H&E -> Reticulin, gen_R translates Reticulin -> H&E.\n",
    "gen_H = UNetGenerator(img_channels=IMG_CHANNELS, features=64).to(DEVICE)\n",
    "gen_R = UNetGenerator(img_channels=IMG_CHANNELS, features=64).to(DEVICE)\n",
    "disc_H = Discriminator(in_channels=IMG_CHANNELS).to(DEVICE)\n",
    "disc_R = Discriminator(in_channels=IMG_CHANNELS).to(DEVICE)\n",
    "logging.info(\"U-Net Generators and Discriminator models initialized and moved to device.\")\n",
    "\n",
    "# Create the optimizers. Adam is the standard choice for GANs.\n",
    "# The `itertools.chain` trick groups both generators' parameters together,\n",
    "# so a single optimizer can update them both based on the combined generator loss.\n",
    "opt_gen = optim.Adam(\n",
    "    itertools.chain(gen_H.parameters(), gen_R.parameters()), \n",
    "    lr=LEARNING_RATE_GEN, \n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "opt_disc_H = optim.Adam(disc_H.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999))\n",
    "opt_disc_R = optim.Adam(disc_R.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999))\n",
    "logging.info(\"Adam optimizers initialized.\")\n",
    "\n",
    "# Create GradScalers for Automatic Mixed Precision (AMP) to prevent\n",
    "# numerical underflow and speed up training.\n",
    "scaler_gen = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "scaler_disc_H = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "scaler_disc_R = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "logging.info(\"GradScalers initialized for AMP.\")\n",
    "\n",
    "\n",
    "# --- 3. Training Utilities ---\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A buffer to store a history of previously generated images.\n",
    "    \n",
    "    Instead of training the discriminator on just the latest batch of fakes,\n",
    "    this buffer provides a mix of recent and slightly older fakes, which\n",
    "    is a well-known technique for stabilizing GAN training.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data_batch):\n",
    "        images_to_return = []\n",
    "        for element in data_batch.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                images_to_return.append(element)\n",
    "            else:\n",
    "                if random.random() > 0.5:\n",
    "                    # With 50% probability, replace an old image and return it\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    images_to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    # With 50% probability, return the new image without replacing\n",
    "                    images_to_return.append(element)\n",
    "        return torch.cat(images_to_return)\n",
    "\n",
    "buffer_fake_H = ReplayBuffer()\n",
    "buffer_fake_R = ReplayBuffer()\n",
    "logging.info(\"Replay buffers initialized.\")\n",
    "\n",
    "\n",
    "# --- Checkpoint Saving & Loading Functions ---\n",
    "\n",
    "def save_checkpoint(model, optimizer, scaler, filename):\n",
    "    \"\"\"Saves model, optimizer, and scaler states to a file.\"\"\"\n",
    "    full_path = os.path.join(CHECKPOINT_SAVE_DIR, filename)\n",
    "    logging.info(f\"Saving checkpoint => {full_path}\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scaler\": scaler.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, full_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, scaler, lr, base_filename, epoch, device):\n",
    "    \"\"\"Loads model, optimizer, and scaler states from a file.\"\"\"\n",
    "    filename = base_filename.replace(\".pth.tar\", f\"_epoch{epoch}.pth.tar\")\n",
    "    full_path = os.path.join(CHECKPOINT_SAVE_DIR, filename)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        logging.warning(f\"Checkpoint not found at: {full_path}\")\n",
    "        return False\n",
    "\n",
    "    logging.info(f\"Loading checkpoint: {full_path}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(full_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        if optimizer and \"optimizer\" in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "        if scaler and \"scaler\" in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "        logging.info(\"Checkpoint loaded successfully.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load checkpoint {full_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- 4. Resume Logic ---\n",
    "\n",
    "# This variable will be updated if a model is loaded successfully.\n",
    "# It tracks the last completed epoch.\n",
    "start_epoch = 0 \n",
    "\n",
    "if LOAD_MODEL:\n",
    "    if EPOCH_TO_LOAD_FROM > 0:\n",
    "        logging.info(f\"\\n--- Attempting to resume training from epoch {EPOCH_TO_LOAD_FROM} ---\")\n",
    "        \n",
    "        # Load all four models. All must succeed to resume.\n",
    "        # NOTE: The base checkpoint filenames must match the new run_id_string from Cell 1\n",
    "        success_gh = load_checkpoint(gen_H, opt_gen, scaler_gen, LEARNING_RATE_GEN, CHECKPOINT_GEN_H_BASE, EPOCH_TO_LOAD_FROM, DEVICE)\n",
    "        success_gr = load_checkpoint(gen_R, opt_gen, scaler_gen, LEARNING_RATE_GEN, CHECKPOINT_GEN_R_BASE, EPOCH_TO_LOAD_FROM, DEVICE)\n",
    "        success_dh = load_checkpoint(disc_H, opt_disc_H, scaler_disc_H, LEARNING_RATE_DISC, CHECKPOINT_DISC_H_BASE, EPOCH_TO_LOAD_FROM, DEVICE)\n",
    "        success_dr = load_checkpoint(disc_R, opt_disc_R, scaler_disc_R, LEARNING_RATE_DISC, CHECKPOINT_DISC_R_BASE, EPOCH_TO_LOAD_FROM, DEVICE)\n",
    "        \n",
    "        if success_gh and success_gr and success_dh and success_dr:\n",
    "            start_epoch = EPOCH_TO_LOAD_FROM\n",
    "            logging.info(f\"--- Resume successful. Training will continue from epoch {start_epoch + 1}. ---\")\n",
    "        else:\n",
    "            logging.error(\"--- Resume failed. One or more checkpoints could not be loaded. Starting from scratch. ---\")\n",
    "            # Force a fresh start if loading fails\n",
    "            LOAD_MODEL = False\n",
    "            start_epoch = 0\n",
    "    else:\n",
    "        logging.info(\"LOAD_MODEL is True, but EPOCH_TO_LOAD_FROM is 0. Starting training from scratch.\")\n",
    "else:\n",
    "    logging.info(\"\\nLOAD_MODEL is False. Starting training from scratch.\")\n",
    "\n",
    "logging.info(f\"Effective starting epoch for training loop: {start_epoch + 1}\")\n",
    "logging.info(\"\\n--- Cell 4 Initialization Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a1f66-b4e8-499f-99eb-60d7b7e40141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 5: The Training Loop\n",
    "# ==============================================================================\n",
    "# This cell contains the main training logic. It orchestrates the data loading,\n",
    "# model training for both generators and discriminators, and periodic saving of\n",
    "# checkpoints, sample images, and performance graphs.\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "# Helper functions called during the training loop.\n",
    "\n",
    "def denormalize(image_tensor):\n",
    "    \"\"\"Converts a tensor from the [-1, 1] range back to [0, 1] for saving.\"\"\"\n",
    "    return torch.clamp((image_tensor + 1.0) / 2.0, 0.0, 1.0)\n",
    "\n",
    "def generate_and_save_samples(gen_H, gen_R, epoch, dataloader):\n",
    "    \"\"\"Generates and saves a sample image translation pair.\"\"\"\n",
    "    if not dataloader:\n",
    "        logging.warning(f\"Epoch {epoch}: Visualization dataloader not available, skipping sample generation.\")\n",
    "        return\n",
    "    \n",
    "    gen_H.eval()\n",
    "    gen_R.eval()\n",
    "    \n",
    "    try:\n",
    "        real_H_raw, real_R_raw = next(iter(dataloader))\n",
    "    except StopIteration:\n",
    "        logging.warning(\"Visualization dataloader exhausted. Cannot generate new samples.\")\n",
    "        return\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Move raw data to GPU and apply GPU-side transforms\n",
    "        real_H = gpu_vis_transform(real_H_raw.to(DEVICE))\n",
    "        real_R = gpu_vis_transform(real_R_raw.to(DEVICE))\n",
    "        \n",
    "        # Generate translations\n",
    "        fake_R = gen_H(real_H)\n",
    "        fake_H = gen_R(real_R)\n",
    "\n",
    "    # Prepare a concatenated image for easy comparison: [Real_H, Fake_R, Real_R, Fake_H]\n",
    "    # We only show the first image from the batch.\n",
    "    combined_image = torch.cat([\n",
    "        denormalize(real_H[0].cpu()),\n",
    "        denormalize(fake_R[0].cpu()),\n",
    "        denormalize(real_R[0].cpu()),\n",
    "        denormalize(fake_H[0].cpu())\n",
    "    ], dim=2) # Concatenate horizontally\n",
    "\n",
    "    save_path = os.path.join(OUTPUT_IMAGE_DIR, f'sample_epoch_{epoch:04d}.png')\n",
    "    save_image(combined_image, save_path)\n",
    "    logging.info(f\"Saved sample image to {save_path}\")\n",
    "\n",
    "    gen_H.train()\n",
    "    gen_R.train()\n",
    "\n",
    "\n",
    "def plot_and_save_metrics(metrics, epoch, save_dir):\n",
    "    \"\"\"Plots the collected loss history and saves it to a file.\"\"\"\n",
    "    # (This function is kept from your previous robust version)\n",
    "    num_epochs = max(len(v) for v in metrics.values()) if metrics else 0\n",
    "    if num_epochs == 0: return\n",
    "\n",
    "    epochs_range = list(range(1, num_epochs + 1))\n",
    "    plot_groups = [\n",
    "        {'title': 'Generator Adversarial Loss', 'keys': ['gen_G_loss']},\n",
    "        {'title': 'Discriminator Loss', 'keys': ['disc_H_loss', 'disc_R_loss']},\n",
    "        {'title': 'Cycle Consistency Loss', 'keys': ['cycle_H_loss', 'cycle_R_loss']},\n",
    "        {'title': 'Identity Loss', 'keys': ['identity_H_loss', 'identity_R_loss']},\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(plot_groups), 1, figsize=(16, 7 * len(plot_groups)), sharex=True)\n",
    "    for i, group in enumerate(plot_groups):\n",
    "        ax = axes[i]\n",
    "        for key in group['keys']:\n",
    "            if key in metrics and any(not np.isnan(v) for v in metrics.get(key, [])):\n",
    "                ax.plot(epochs_range, metrics[key], marker='o', linestyle='-', markersize=4, label=key)\n",
    "        \n",
    "        tick_step = 5 if num_epochs >= 50 else (2 if num_epochs > 20 else 1)\n",
    "        ax.set_xticks([e for e in epochs_range if e % tick_step == 0 or e == 1])\n",
    "        ax.set_title(group['title'], fontsize=14)\n",
    "        ax.set_ylabel(\"Loss\"); ax.legend(); ax.grid(True)\n",
    "        \n",
    "    axes[-1].set_xlabel(\"Epochs\")\n",
    "    fig.suptitle(f'Training Metrics for {num_epochs} Epochs', fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    save_path = os.path.join(save_dir, f'metrics_graph_epoch_{epoch:04d}.png')\n",
    "    plt.savefig(save_path); plt.close(fig)\n",
    "    logging.info(f\"Saved metrics graph to {save_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MODULAR TRAINING FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def train_discriminators(real_H, real_R, gen_H, gen_R, disc_H, disc_R, opt_disc_H, opt_disc_R, scaler_disc_H, scaler_disc_R, buffer_fake_H, buffer_fake_R):\n",
    "    \"\"\"Performs one training step for both discriminators.\"\"\"\n",
    "    losses = {}\n",
    "    \n",
    "    # --- Train Discriminator H (for H&E images) ---\n",
    "    with torch.cuda.amp.autocast():\n",
    "        fake_H = gen_R(real_R)\n",
    "        D_H_real = disc_H(real_H)\n",
    "        D_H_fake = disc_H(buffer_fake_H.push_and_pop(fake_H.detach()))\n",
    "        D_H_real_loss = adv_loss_fn(D_H_real, torch.ones_like(D_H_real))\n",
    "        D_H_fake_loss = adv_loss_fn(D_H_fake, torch.zeros_like(D_H_fake))\n",
    "        D_H_loss = (D_H_real_loss + D_H_fake_loss) / 2\n",
    "    \n",
    "    opt_disc_H.zero_grad()\n",
    "    scaler_disc_H.scale(D_H_loss).backward()\n",
    "    scaler_disc_H.step(opt_disc_H)\n",
    "    scaler_disc_H.update()\n",
    "    losses['disc_H_loss'] = D_H_loss.item()\n",
    "    \n",
    "    # --- Train Discriminator R (for Reticulin images) ---\n",
    "    with torch.cuda.amp.autocast():\n",
    "        fake_R = gen_H(real_H)\n",
    "        D_R_real = disc_R(real_R)\n",
    "        D_R_fake = disc_R(buffer_fake_R.push_and_pop(fake_R.detach()))\n",
    "        D_R_real_loss = adv_loss_fn(D_R_real, torch.ones_like(D_R_real))\n",
    "        D_R_fake_loss = adv_loss_fn(D_R_fake, torch.zeros_like(D_R_fake))\n",
    "        D_R_loss = (D_R_real_loss + D_R_fake_loss) / 2\n",
    "        \n",
    "    opt_disc_R.zero_grad()\n",
    "    scaler_disc_R.scale(D_R_loss).backward()\n",
    "    scaler_disc_R.step(opt_disc_R)\n",
    "    scaler_disc_R.update()\n",
    "    losses['disc_R_loss'] = D_R_loss.item()\n",
    "    \n",
    "    return losses, fake_H, fake_R # Return fakes for generator training\n",
    "\n",
    "def train_generators(real_H, real_R, fake_H, fake_R, gen_H, gen_R, disc_H, disc_R, opt_gen, scaler_gen):\n",
    "    \"\"\"Performs one training step for both generators.\"\"\"\n",
    "    losses = {}\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        # --- Adversarial Loss ---\n",
    "        D_H_fake = disc_H(fake_H)\n",
    "        D_R_fake = disc_R(fake_R)\n",
    "        loss_G_H_adv = adv_loss_fn(D_H_fake, torch.ones_like(D_H_fake))\n",
    "        loss_G_R_adv = adv_loss_fn(D_R_fake, torch.ones_like(D_R_fake))\n",
    "        \n",
    "        # --- Cycle Consistency Loss ---\n",
    "        cycled_H = gen_R(fake_R)\n",
    "        loss_cycle_H = cycle_loss_fn(real_H, cycled_H)\n",
    "        cycled_R = gen_H(fake_H)\n",
    "        loss_cycle_R = cycle_loss_fn(real_R, cycled_R)\n",
    "        \n",
    "        # --- Identity Loss ---\n",
    "        if LAMBDA_IDENTITY > 0:\n",
    "            identity_H = gen_R(real_H)\n",
    "            loss_identity_H = identity_loss_fn(real_H, identity_H)\n",
    "            identity_R = gen_H(real_R)\n",
    "            loss_identity_R = identity_loss_fn(real_R, identity_R)\n",
    "            losses['identity_H_loss'] = loss_identity_H.item()\n",
    "            losses['identity_R_loss'] = loss_identity_R.item()\n",
    "        else:\n",
    "            loss_identity_H = loss_identity_R = 0\n",
    "            \n",
    "        # --- Total Generator Loss ---\n",
    "        total_G_loss = (\n",
    "            loss_G_H_adv + loss_G_R_adv +\n",
    "            (loss_cycle_H * LAMBDA_CYCLE) +\n",
    "            (loss_cycle_R * LAMBDA_CYCLE) +\n",
    "            (loss_identity_H * LAMBDA_IDENTITY) +\n",
    "            (loss_identity_R * LAMBDA_IDENTITY)\n",
    "        )\n",
    "    \n",
    "    opt_gen.zero_grad()\n",
    "    scaler_gen.scale(total_G_loss).backward()\n",
    "    scaler_gen.step(opt_gen)\n",
    "    scaler_gen.update()\n",
    "\n",
    "    losses['gen_G_loss'] = total_G_loss.item()\n",
    "    losses['cycle_H_loss'] = loss_cycle_H.item()\n",
    "    losses['cycle_R_loss'] = loss_cycle_R.item()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAIN TRAINING EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- GPU-side Transforms ---\n",
    "gpu_train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), antialias=True),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "gpu_vis_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), antialias=True),\n",
    "])\n",
    "\n",
    "# --- Metrics History Initialization ---\n",
    "# Clear or initialize history at the start of a training run.\n",
    "history_keys = [\"gen_G_loss\", \"disc_H_loss\", \"disc_R_loss\", \"cycle_H_loss\", \"cycle_R_loss\", \"identity_H_loss\", \"identity_R_loss\"]\n",
    "metrics_history = {key: [] for key in history_keys}\n",
    "logging.info(\"Initialized metrics history for this run.\")\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "logging.info(f\"\\n--- Starting Training Loop from Epoch {start_epoch + 1} ---\")\n",
    "if train_dataloader is None:\n",
    "    raise RuntimeError(\"CRITICAL ERROR: train_dataloader is None. Training cannot start.\")\n",
    "\n",
    "for epoch_iter_idx in range(NUM_EPOCHS - start_epoch):\n",
    "    current_epoch = start_epoch + epoch_iter_idx + 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Dictionaries to aggregate losses for the epoch summary\n",
    "    epoch_losses_sum = {key: 0.0 for key in history_keys}\n",
    "    \n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch [{current_epoch}/{NUM_EPOCHS}]\", leave=True)\n",
    "\n",
    "    for batch_idx, (real_H_raw, real_R_raw) in enumerate(loop):\n",
    "        # Move raw data to GPU and apply transforms\n",
    "        real_H = gpu_train_transform(real_H_raw.to(DEVICE))\n",
    "        real_R = gpu_train_transform(real_R_raw.to(DEVICE))\n",
    "        \n",
    "        # --- Run one training step ---\n",
    "        disc_losses, fake_H, fake_R = train_discriminators(real_H, real_R, gen_H, gen_R, disc_H, disc_R, opt_disc_H, opt_disc_R, scaler_disc_H, scaler_disc_R, buffer_fake_H, buffer_fake_R)\n",
    "        gen_losses = train_generators(real_H, real_R, fake_H, fake_R, gen_H, gen_R, disc_H, disc_R, opt_gen, scaler_gen)\n",
    "        \n",
    "        # --- Update tracking and progress bar ---\n",
    "        all_losses = {**disc_losses, **gen_losses}\n",
    "        for key, value in all_losses.items():\n",
    "            epoch_losses_sum[key] += value\n",
    "        \n",
    "        loop.set_postfix(G=gen_losses['gen_G_loss'], D=disc_losses['disc_H_loss'] + disc_losses['disc_R_loss'], refresh=True)\n",
    "    \n",
    "    loop.close()\n",
    "    \n",
    "    # --- End of Epoch Actions ---\n",
    "    \n",
    "    # 1. Log average losses\n",
    "    for key in history_keys:\n",
    "        avg_loss = epoch_losses_sum[key] / len(train_dataloader)\n",
    "        metrics_history[key].append(avg_loss)\n",
    "    \n",
    "    avg_loss_str = \" \".join([f\"{k.replace('_loss', '')}:{v[-1]:.3f}\" for k, v in metrics_history.items() if v])\n",
    "    logging.info(f\"Epoch [{current_epoch}/{NUM_EPOCHS}] Summary | Time: {time.time()-start_time:.1f}s | Avgs: [{avg_loss_str}]\")\n",
    "    \n",
    "    # 2. Save sample images\n",
    "    if current_epoch % SAVE_SAMPLES_EVERY_N_EPOCHS == 0 or current_epoch == 1:\n",
    "        generate_and_save_samples(gen_H, gen_R, current_epoch, vis_dataloader)\n",
    "    \n",
    "    # 3. Plot metrics\n",
    "    if current_epoch % CALC_METRICS_EVERY_N_EPOCHS == 0:\n",
    "        plot_and_save_metrics(metrics_history, current_epoch, OUTPUT_IMAGE_DIR)\n",
    "\n",
    "    # 4. Save model checkpoints\n",
    "    if SAVE_MODEL and (current_epoch % SAVE_MODEL_EVERY_N_EPOCHS == 0):\n",
    "        save_checkpoint(gen_H, opt_gen, scaler_gen, CHECKPOINT_GEN_H_BASE.replace(\".pth.tar\", f\"_epoch{current_epoch}.pth.tar\"))\n",
    "        save_checkpoint(gen_R, opt_gen, scaler_gen, CHECKPOINT_GEN_R_BASE.replace(\".pth.tar\", f\"_epoch{current_epoch}.pth.tar\"))\n",
    "        save_checkpoint(disc_H, opt_disc_H, scaler_disc_H, CHECKPOINT_DISC_H_BASE.replace(\".pth.tar\", f\"_epoch{current_epoch}.pth.tar\"))\n",
    "        save_checkpoint(disc_R, opt_disc_R, scaler_disc_R, CHECKPOINT_DISC_R_BASE.replace(\".pth.tar\", f\"_epoch{current_epoch}.pth.tar\"))\n",
    "    \n",
    "    # 5. Check for stop signal (for graceful pausing)\n",
    "    stop_file_path = os.path.join(NOTEBOOK_CWD, \"stop_training.txt\")\n",
    "    if os.path.exists(stop_file_path):\n",
    "        logging.info(f\"\\n--- Stop file detected. Saving final model at epoch {current_epoch} and stopping. ---\")\n",
    "        # Perform one last save\n",
    "        save_checkpoint(gen_H, opt_gen, scaler_gen, CHECKPOINT_GEN_H_BASE.replace(\".pth.tar\", f\"_epoch{current_epoch}.pth.tar\"))\n",
    "        # ... (add saves for other 3 models)\n",
    "        os.remove(stop_file_path)\n",
    "        logging.info(\"--- Training paused gracefully. ---\")\n",
    "        break\n",
    "\n",
    "logging.info(\"\\n--- Training Complete! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
